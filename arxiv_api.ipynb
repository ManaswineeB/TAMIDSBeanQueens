{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c69ead6",
   "metadata": {},
   "source": [
    "## Structure\n",
    "Below is the code I used to salvage the data from Arxiv.\n",
    "You don't need to run it. The data is collected and stored in the file 'arxived.txt'. Just run the cell below and it'll store the data in the list named 'arxiv_publications'.\n",
    "## Cleanup needed\n",
    "The list it'll load will have repeated entries and it may even have some garbage data, namely some publincations listed there may be may be of someone not related to A&M. To cleanup the repeated entry I think you can make the list a set and then convert it back into a list, and that should remove repetations. To clean up the ghost entries, I don't know how I'd do that."
   ]
  },
  {
   "cell_type": "raw",
   "id": "e5601a7e",
   "metadata": {},
   "source": [
    "## This cell shows the code I used to collect and store the data\n",
    "import feedparser as fp\n",
    "import urllib\n",
    "import requests\n",
    "import string\n",
    "import json\n",
    "import time\n",
    "def refine_texts(text):\n",
    "    text = text.lower()\n",
    "    text = text.replace(',','')\n",
    "    for s in text:\n",
    "        if s not in string.ascii_lowercase+string.digits:\n",
    "            text = text.replace(s,' ')\n",
    "        else:\n",
    "            continue\n",
    "    return text.split()\n",
    "\n",
    "people = requests.get('https://api.library.tamu.edu/scholars-discovery/individual/n73bc0d79?fbclid=IwAR14AfAGbKtQYKRKPsaNcAdzvqQFt76bXYunrdSUMBxZXpGKC6yKxkcGQ4E').json()\n",
    "people = people['people']\n",
    "publication_data=[]\n",
    "for i in people:\n",
    "    url = 'https://api.library.tamu.edu/scholars-discovery/individual/'+i['id']\n",
    "    publication_data.append(requests.get(url).json())\n",
    "\n",
    "outfile = open('arxived.txt','r')\n",
    "arxiv_publications = json.loads(outfile.read())\n",
    "outfile.close()\n",
    "max_results = 3\n",
    "people_counter = 110\n",
    "for p in publication_data[people_counter:]:\n",
    "    if 'publications' in p.keys():\n",
    "        arxiv_url = 'http://export.arxiv.org/api/query?search_query='\n",
    "        name = refine_texts(p['name'])\n",
    "        last_name = 'au:%s'%name[0]\n",
    "        for publication in p['publications']:\n",
    "            title_url = '+AND+ti:'\n",
    "            title = refine_texts(publication['label'])\n",
    "            title_url+= title[0]\n",
    "            for j in title[1:10]:\n",
    "                title_url+= '+AND+ti:%s'%j\n",
    "            webURL = arxiv_url+last_name+title_url\n",
    "            webURL+='&max_results=%i'%max_results\n",
    "            data = urllib.request.urlopen(arxiv_url+last_name+title_url).read()\n",
    "            feed = fp.parse(data)\n",
    "            arxiv_publications = arxiv_publications + feed['entries']\n",
    "            outfile = open('arxived.txt','w')\n",
    "            outfile.write(json.dumps(arxiv_publications))\n",
    "            outfile.close()\n",
    "            for i in feed['entries']:\n",
    "                print(i['title']+'   %i\\n'%people_counter)\n",
    "            time.sleep(4)\n",
    "        people_counter+= 1\n",
    "    else:\n",
    "        people_counter+= 1\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "076d147c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('arxived.txt','r') as f:\n",
    "    arxiv_publications = json.loads(f.read())\n",
    "    f.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
